{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (0.6.9)\n",
      "Requirement already satisfied: dataclasses-json in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from llama-index) (0.5.7)\n",
      "Requirement already satisfied: langchain>=0.0.154 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from llama-index) (0.0.170)\n",
      "Requirement already satisfied: numpy in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from llama-index) (1.24.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from llama-index) (8.2.2)\n",
      "Requirement already satisfied: openai>=0.26.4 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from llama-index) (0.27.5)\n",
      "Requirement already satisfied: pandas in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from llama-index) (2.0.1)\n",
      "Requirement already satisfied: requests<2.30.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from llama-index) (2.29.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from llama-index) (2023.5.0)\n",
      "Requirement already satisfied: tiktoken in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from llama-index) (0.3.3)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from langchain>=0.0.154->llama-index) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from langchain>=0.0.154->llama-index) (1.4.48)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from langchain>=0.0.154->llama-index) (3.8.4)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from langchain>=0.0.154->llama-index) (2.8.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from langchain>=0.0.154->llama-index) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from langchain>=0.0.154->llama-index) (1.10.7)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from openai>=0.26.4->llama-index) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from requests<2.30.0->llama-index) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from requests<2.30.0->llama-index) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from requests<2.30.0->llama-index) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from requests<2.30.0->llama-index) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from pandas->llama-index) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from pandas->llama-index) (2023.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from tiktoken->llama-index) (2023.3.23)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (1.3.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama-index) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from pydantic<2,>=1->langchain>=0.0.154->llama-index) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from typing-inspect>=0.4.0->dataclasses-json->llama-index) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: slack_sdk in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (3.21.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: qdrant_client in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (1.1.7)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from qdrant_client) (1.54.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from qdrant_client) (1.54.0)\n",
      "Requirement already satisfied: httpx[http2]>=0.14.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from qdrant_client) (0.24.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from qdrant_client) (1.24.3)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from qdrant_client) (2.7.0)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.8 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from qdrant_client) (1.10.7)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.0.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from qdrant_client) (4.5.0)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.26.14 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from qdrant_client) (1.26.15)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant_client) (4.23.0)\n",
      "Requirement already satisfied: setuptools in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant_client) (67.7.2)\n",
      "Requirement already satisfied: certifi in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from httpx[http2]>=0.14.0->qdrant_client) (2022.12.7)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from httpx[http2]>=0.14.0->qdrant_client) (0.17.0)\n",
      "Requirement already satisfied: idna in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from httpx[http2]>=0.14.0->qdrant_client) (3.4)\n",
      "Requirement already satisfied: sniffio in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from httpx[http2]>=0.14.0->qdrant_client) (1.3.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from httpx[http2]>=0.14.0->qdrant_client) (4.1.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant_client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant_client) (4.0.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from httpcore<0.18.0,>=0.15.0->httpx[http2]>=0.14.0->qdrant_client) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages (from httpcore<0.18.0,>=0.15.0->httpx[http2]>=0.14.0->qdrant_client) (3.6.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index\n",
    "!pip install slack_sdk\n",
    "!pip install qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alfie/Documents/code/hash/apps/hash-agents/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from llama_index import GPTListIndex, SlackReader\n",
    "\n",
    "import logging\n",
    "import structlog\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "load_dotenv()\n",
    "load_dotenv(dotenv_path=find_dotenv(filename=\".env.local\"), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"urllib3.util.retry\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"openai\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(10))\n",
    "def create_embedding_with_backoff(**kwargs):\n",
    "    return openai.Embedding.create(**kwargs)\n",
    "\n",
    "def create_embedding(message, num: int | None, total: int | None):\n",
    "    logger.debug(\"Creating embedding\", num=num, total=total)\n",
    "    embedding = create_embedding_with_backoff(\n",
    "        input=json.dumps(message),\n",
    "        model=embedding_model,\n",
    "    )[\"data\"][0][\"embedding\"]\n",
    "    return {\n",
    "        \"embedding\": embedding,\n",
    "        \"message\": message,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (1168879067.py, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 49\u001b[0;36m\u001b[0m\n\u001b[0;31m    return NotionBlockResult(content=\" \".join(cur_result_text_arr), block_ids, sub_pages)\u001b[0m\n\u001b[0m                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "from queue import Queue\n",
    "from collections import namedtuple\n",
    "from concurrent.futures import Future, ThreadPoolExecutor\n",
    "\n",
    "from notion_client import Client\n",
    "from notion_client.helpers import collect_paginated_api\n",
    "\n",
    "NotionBlockResult = namedtuple(\"NotionBlockResult\", [\"content\", \"block_ids\", \"sub_pages\"])\n",
    "\n",
    "def traverse_notion_block(\n",
    "    notion_client: Client,\n",
    "    task_queue: Queue,\n",
    "    block_id,\n",
    ") -> NotionBlockResult:\n",
    "    print(f\"Getting children blocks of {block_id}\")\n",
    "    \n",
    "    paginated_contents = collect_paginated_api(\n",
    "        notion_client.blocks.children.list,\n",
    "        block_id=block_id,\n",
    "    )\n",
    "\n",
    "    block_ids = []\n",
    "    sub_pages = []\n",
    "\n",
    "    for page in paginated_contents:\n",
    "        result_obj = page[page[\"type\"]]\n",
    "\n",
    "        cur_result_text_arr = []\n",
    "        if \"rich_text\" in result_obj:\n",
    "            for rich_text in result_obj[\"rich_text\"]:\n",
    "                # skip if doesn't have text object\n",
    "                if \"text\" in rich_text:\n",
    "                    text = rich_text[\"text\"][\"content\"]\n",
    "                    cur_result_text_arr.append(text)\n",
    "                elif \"plain_text\" in rich_text:\n",
    "                    text = rich_text[\"plain_text\"]\n",
    "                    cur_result_text_arr.append(text)\n",
    "        elif \"title\" in result_obj:\n",
    "            text = result_obj[\"title\"]\n",
    "            cur_result_text_arr.append(text)\n",
    "\n",
    "        if page[\"has_children\"]:\n",
    "            task_queue.put(page[\"id\"])\n",
    "            if page[\"type\"] == \"child_page\":\n",
    "                sub_pages.append(page[\"id\"])\n",
    "            else:\n",
    "                block_ids.append(page[\"id\"])\n",
    "\n",
    "    return NotionBlockResult(content=\" \".join(cur_result_text_arr), block_ids, sub_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import concurrent.futures\n",
    "import queue\n",
    "import os\n",
    "import threading\n",
    "from notion_client import Client\n",
    "\n",
    "integration_token = os.getenv(\"NOTION_API_KEY\")\n",
    "page_ids = [\"964db53ace9940519613dce0485820fd\"]\n",
    "\n",
    "notion_client = Client(auth=integration_token) \n",
    "\n",
    "executor = concurrent.futures.ThreadPoolExecutor()\n",
    "task_queue = queue.Queue()\n",
    "results = {}\n",
    "\n",
    "def process_queue():\n",
    "    while True:\n",
    "        block_id = task_queue.get()\n",
    "        results[block_id] = executor.submit(traverse_notion_block, notion_client, task_queue, block_id)\n",
    "        task_queue.task_done()\n",
    "\n",
    "queue_thread = threading.Thread(target=process_queue)\n",
    "queue_thread.start()\n",
    "\n",
    "for page_id in page_ids:\n",
    "    task_queue.put(page_id)\n",
    "\n",
    "task_queue.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_to_explore = [*page_ids]\n",
    "explored_pages = set()\n",
    "pages_to_contents = {}\n",
    "\n",
    "\n",
    "def collapse_text(root_block_id, depth):\n",
    "    block_result = results[root_block_id].result()\n",
    "    pages_to_explore.extend(block_result.sub_pages)\n",
    "\n",
    "    block_content = block_result.content\n",
    "    nested_content = \"\\n\".join(\n",
    "        [collapse_text(block_id, depth + 1) for block_id in block_result.block_ids]\n",
    "    )\n",
    "    return (depth * \"\\t\") + block_content + nested_content\n",
    "\n",
    "\n",
    "while len(pages_to_explore) > 0:\n",
    "    page_id = pages_to_explore.pop(0)\n",
    "    if page_id in explored_pages:\n",
    "        continue\n",
    "    explored_pages.add(page_id)\n",
    "\n",
    "    print(f\"Exploring page {page_id}\")\n",
    "    pages_to_contents[page_id] = collapse_text(page_id, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_payload(document):\n",
    "    return {\n",
    "        \"text\": document.text if document.text else None,\n",
    "        \"doc_id\": document.doc_id if document.doc_id else None,\n",
    "        \"extra_info\": document.extra_info if document.extra_info else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = create_embedding(message=create_payload(documents[0]), num=0, total=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    embeddings = list(\n",
    "        executor.map(\n",
    "            lambda idx_item: create_embedding(\n",
    "                message=create_payload(idx_item[1]),\n",
    "                num=idx_item[0],\n",
    "                total=len(documents),\n",
    "            ),\n",
    "            enumerate(documents),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import LLMPredictor, ServiceContext, PromptHelper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "prompt_helper = PromptHelper.from_llm_predictor(llm_predictor=llm_predictor)\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    chunk_size_limit=1024,\n",
    "    prompt_helper=prompt_helper,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTQdrantIndex\n",
    "import qdrant_client\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    url=\"http://localhost:6333\"\n",
    ")\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"My docker-compose isn't working, how can I fix it?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
