{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install slack_sdk\n",
    "!pip install qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from llama_index import GPTListIndex, SlackReader\n",
    "\n",
    "import logging\n",
    "import structlog\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "load_dotenv()\n",
    "load_dotenv(dotenv_path=find_dotenv(filename=\".env.local\"), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"urllib3.util.retry\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"openai\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(10))\n",
    "def create_embedding_with_backoff(**kwargs):\n",
    "    return openai.Embedding.create(**kwargs)\n",
    "\n",
    "def create_embedding(message, num: int | None, total: int | None):\n",
    "    logger.debug(\"Creating embedding\", num=num, total=total)\n",
    "    embedding = create_embedding_with_backoff(\n",
    "        input=json.dumps(message),\n",
    "        model=embedding_model,\n",
    "    )[\"data\"][0][\"embedding\"]\n",
    "    return {\n",
    "        \"embedding\": embedding,\n",
    "        \"message\": message,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from queue import Queue\n",
    "\n",
    "from attr import dataclass\n",
    "from notion_client import Client\n",
    "from notion_client.helpers import collect_paginated_api\n",
    "\n",
    "NotionBlockResult = namedtuple(\n",
    "    \"NotionBlockResult\",\n",
    "    [\"content_components\", \"sub_pages\"],\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class NotionBlockContent:\n",
    "    contents: str\n",
    "    inline: bool\n",
    "\n",
    "@dataclass\n",
    "class NotionBlockIdentifier:\n",
    "    identifier: str\n",
    "\n",
    "NotionBlockContentOrIdentifier = NotionBlockContent | NotionBlockIdentifier\n",
    "\n",
    "def traverse_notion_block(\n",
    "    notion_client: Client,\n",
    "    task_queue: Queue,\n",
    "    block_id,\n",
    ") -> NotionBlockResult:\n",
    "    print(f\"Getting children blocks of {block_id}\")\n",
    "\n",
    "    block_children = collect_paginated_api(\n",
    "        notion_client.blocks.children.list,\n",
    "        block_id=block_id,\n",
    "    )\n",
    "\n",
    "    sub_pages = []\n",
    "    content_components: list[NotionBlockContentOrIdentifier] = []\n",
    "\n",
    "    for block_child in block_children:\n",
    "        result_obj = block_child[block_child[\"type\"]]\n",
    "\n",
    "        is_subpage = block_child[\"type\"] == \"child_page\"\n",
    "        \n",
    "        if \"rich_text\" in result_obj:\n",
    "            for rich_text in result_obj[\"rich_text\"]:\n",
    "                # skip if doesn't have text object\n",
    "                if \"text\" in rich_text:\n",
    "                    text = rich_text[\"text\"][\"content\"]\n",
    "                    content_components.append(NotionBlockContent(contents=text, inline=True))\n",
    "                elif \"plain_text\" in rich_text:\n",
    "                    text = rich_text[\"plain_text\"]\n",
    "                    content_components.append(NotionBlockContent(contents=text, inline=True))\n",
    "        elif \"title\" in result_obj:\n",
    "            text = result_obj[\"title\"]\n",
    "            if is_subpage:\n",
    "                content_components.append(NotionBlockContent(contents=text, inline=False))\n",
    "                # Kind of hacky but otherwise we don't get a newline after the subpage\n",
    "                content_components.append(NotionBlockContent(contents=\"\", inline=False))\n",
    "            else:\n",
    "                content_components.append(NotionBlockContent(contents=text, inline=True))\n",
    "            \n",
    "        if block_child[\"has_children\"]:\n",
    "            task_queue.put(block_child[\"id\"])\n",
    "            if is_subpage:\n",
    "                sub_pages.append(block_child[\"id\"])\n",
    "            else:\n",
    "                content_components.append(NotionBlockIdentifier(block_child[\"id\"]))\n",
    "        else:\n",
    "            # Kind of hacky but otherwise we're not getting newlines between text blocks\n",
    "            content_components.append(NotionBlockContent(contents=\"\", inline=False))\n",
    "\n",
    "    return NotionBlockResult(content_components=content_components, sub_pages=sub_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "from notion_client import Client\n",
    "\n",
    "integration_token = os.getenv(\"NOTION_API_KEY\")\n",
    "page_ids = [\"52b92f24e14b43be83f64c206f211413\"]\n",
    "\n",
    "notion_client = Client(auth=integration_token)\n",
    "\n",
    "executor = concurrent.futures.ThreadPoolExecutor()\n",
    "task_queue = queue.Queue()\n",
    "future_queue = queue.Queue()\n",
    "results = {}\n",
    "\n",
    "\n",
    "def process_task_queue():\n",
    "    while True:\n",
    "        block_id = task_queue.get()\n",
    "        future = executor.submit(\n",
    "            traverse_notion_block,\n",
    "            notion_client,\n",
    "            task_queue,\n",
    "            block_id,\n",
    "        )\n",
    "        results[block_id] = future\n",
    "        future_queue.put(future)\n",
    "        task_queue.task_done()\n",
    "\n",
    "# Hacky but if we start a second thread to ensure that all the futures are complete, we avoid race conditions that occur \n",
    "# from the interactions of executor.submit and task_queue.task_done\n",
    "def process_future_queue():\n",
    "    while True:\n",
    "        future = future_queue.get()\n",
    "        future.result()\n",
    "        future_queue.task_done()\n",
    "\n",
    "task_queue_thread = threading.Thread(target=process_task_queue)\n",
    "task_queue_thread.start()\n",
    "future_queue_thread = threading.Thread(target=process_future_queue)\n",
    "future_queue_thread.start()\n",
    "\n",
    "\n",
    "for page_id in page_ids:\n",
    "    task_queue.put(page_id)\n",
    "\n",
    "task_queue.join()\n",
    "future_queue.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_to_explore = [*page_ids]\n",
    "explored_pages = set()\n",
    "pages_to_contents = {}\n",
    "\n",
    "\n",
    "def collapse_text(root_block_id, depth):\n",
    "    block_result = results[root_block_id].result()\n",
    "    pages_to_explore.extend(block_result.sub_pages)\n",
    "\n",
    "    indent = (depth * \"\\t\")\n",
    "    block_contents = [indent]\n",
    "    \n",
    "    for content_component in block_result.content_components:\n",
    "        if isinstance(content_component, NotionBlockIdentifier):\n",
    "            block_contents.append(\"\\n\" + indent)\n",
    "            block_contents.append(collapse_text(content_component.identifier, depth + 1))\n",
    "        else:\n",
    "            if not content_component.inline:\n",
    "                block_contents.append(\"\\n\" + indent)\n",
    "            \n",
    "            block_contents.append(content_component.contents)\n",
    "    \n",
    "    return \"\".join(block_contents)\n",
    "\n",
    "while len(pages_to_explore) > 0:\n",
    "    page_id = pages_to_explore.pop(0)\n",
    "    if page_id in explored_pages:\n",
    "        continue\n",
    "    explored_pages.add(page_id)\n",
    "\n",
    "    print(f\"Collapsing text for page {page_id}\")\n",
    "    pages_to_contents[page_id] = collapse_text(page_id, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_to_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=30,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "documents = []\n",
    "\n",
    "for page_id, contents in pages_to_contents.items():\n",
    "    chunks = text_splitter.split_text(contents)\n",
    "    for chunk_index, chunk in enumerate(chunks):\n",
    "        documents.append({\"text\": chunk, \"metadata\": {\"parent_page_id\": page_id, \"chunk_idx\": chunk_index}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_payload(document):\n",
    "    return {\n",
    "        \"text\": document.text if document.text else None,\n",
    "        \"doc_id\": document.doc_id if document.doc_id else None,\n",
    "        \"extra_info\": document.extra_info if document.extra_info else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = create_embedding(message=create_payload(documents[0]), num=0, total=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    embeddings = list(\n",
    "        executor.map(\n",
    "            lambda idx_item: create_embedding(\n",
    "                message=create_payload(idx_item[1]),\n",
    "                num=idx_item[0],\n",
    "                total=len(documents),\n",
    "            ),\n",
    "            enumerate(documents),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import LLMPredictor, ServiceContext, PromptHelper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "prompt_helper = PromptHelper.from_llm_predictor(llm_predictor=llm_predictor)\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    chunk_size_limit=1024,\n",
    "    prompt_helper=prompt_helper,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTQdrantIndex\n",
    "import qdrant_client\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    url=\"http://localhost:6333\"\n",
    ")\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"My docker-compose isn't working, how can I fix it?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
